# Deep Learning Module Overview

This module explores both foundational and advanced deep learning concepts, connecting theory to practical applications across diverse domains. You will gain hands-on experience with state-of-the-art techniques and develop the skills needed for research and industry roles in deep learning.

## Coursework

- **Coursework 1:** Design, train, and optimize a custom deep learning model for classifying a curated subset of ImageNet, called NaturalImageNet. This dataset includes 100 food classes from food pictures scrapped from Internet. **(100%)**
- **Coursework 2:** Implement two popular generative models: a Variational Autoencoder (VAE) and a Denoising Diffussion Probablistic Model (DDPM) to build a hotdog generator. **(99%)**

## Weekly Notes and Resources

| Week | Topics                                | Notes                                        | Lecture Notes                                                            | Tutorials                      | Papers                |
|------|---------------------------------------|----------------------------------------------|--------------------------------------------------------------------------|-------------------------------|-----------------------|
| 2    | Curse of Dimensionality, CNNs         | [[Week 2]]                                   | [[N01_Convolutions.pdf]]                                                 | [[Tutorial 1.pdf]]             |                       |
| 3    | Equivariance, Invariance, LeNet, AlexNet, VGG | [[Week 3 - 02]], [[Week 3 - 03]], [[Week 3 - 04]], [[Week 3 - 05]] | [[N02_equivariance.pdf]], [[N03_LeNet.pdf]], [[N04_AlexNet.pdf]], [[N05_VGG notes.pdf]] | [[Tutorial 3.pdf]]             | [[LeNet_paper.pdf]], [[AlexNet_paper.pdf]], [[VGG.pdf]] |
| 4    | Inception, Batch Norm, ResNet, Activation & Loss Functions, Data Augmentation, U-Net | [[Week 4 - 06]] to [[Week 4 - 12]]           | [[N06_Inception.pdf]], [[N07_BatchNorm.pdf.pdf]], [[N08_ResNet.pdf.pdf]], [[N09_activations.pdf]], [[N10_losses.pdf.pdf]], [[N11_augmentation.pdf.pdf]] | [[T04_covariateShift.pdf.pdf]] | [[Inception.pdf]], [[ResNet.pdf]], [[BatchNorm.pdf]]     |
| 5    | Generative Models, VAE, GANs, Advances | [[Week 5 - Generative Models]], [[Week 5 - VAE Basics]], [[Week 5 - GANs]], [[Week 5 - Advances and Applications]] | [[N15_generative_models_intro_slides.pdf.pdf]], [[N16_VAE.pdf.pdf]], [[N16a_generative_models_vae_basics_slides.pdf.pdf]], [[N17_GAN.pdf.pdf]], [[N17a_generative_models_gan_basics_slides.pdf.pdf]], [[N17b_generative_models_advance_slides.pdf.pdf]] | [[T06_VAEsandGANs.pdf]]        | [[VAEs.pdf]], [[GANs.pdf]] |
| 6    | RNNs, Applications, Attention & Transformers | [[Week 6 - RNNs]], [[Week 6 - RNNs Applications]], [[Week 6 - Attention & Transformers Basics]], [[Week 6 - Advances and Applications]] | [[N18_RNN.pdf]], [[N18a_recurrent_neural_networks_basics_slides.pdf.pdf]], [[N19_recurrent_neural_networks_applications_slides.pdf.pdf]], [[N20_attention.pdf]], [[N20a_attention_basics_slides.pdf.pdf]], [[N21_attention_advance_slides.pdf.pdf]] |                               |                       |

> **Note:** All resources are linked for quick access. Refer to the relevant week for detailed lecture notes, tutorials, and key papers.

